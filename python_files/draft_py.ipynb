{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, LongType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.pandas as pd\n",
    "import json\n",
    "# create our spark context to create dataframe based on json parsing \n",
    "jar = '/workspace/nba_sas_assessment/config/jar/postgresql-42.5.1.jar'\n",
    "\n",
    "sparkClassPath = os.getenv('SPARK_CLASSPATH', jar)\n",
    "\n",
    "\n",
    "spark = (SparkSession.builder.config('spark.jars', f'file:{sparkClassPath}').config('spark.executor.extraClassPath', sparkClassPath).config('spark.driver.extraClassPath', sparkClassPath).appName(\"PySpark processing NBA data\")\\\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('./workspace/nba_sas_assessment/index(6).env')\n",
    "URL = os.getenv('JDBC')\n",
    "USER = os.getenv('PGUSER')\n",
    "PASS = os.getenv('PGPASSWORD')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script_etl import extract_files\n",
    "urls_list = ['https://github.com/sealneaward/nba-movement-data/raw/master/data/01.01.2016.CHA.at.TOR.7z']\n",
    "\n",
    "\n",
    "# Extract the game files from raw\n",
    "game_files = extract_files(folder_raw=folder_raw,folder_tmp_path=folder_tmp_path,urls=urls_list)     \n",
    "\n",
    "\n",
    "## Replace the next variables with your directory of preference ##\n",
    "folder_raw = '/workspace/nba_sas_assessment/raw_data' # directory to save and process raw data\n",
    "folder_tmp_path = \"/workspace/nba_sas_assessment/raw_data/tmp\" # directory to store json raw data\n",
    "\n",
    "## Replace the next variables with the path to postgres jar file and .env with credencials to access neon.tech ##\n",
    "jar_postgres_path = '/workspace/nba_sas_assessment/config/jar/postgresql-42.5.1.jar'\n",
    "env_postgres_credentials = './workspace/nba_sas_assessment/config/postgres_login.env'\n",
    "\n",
    "\n",
    "# use python open method to open json file on read mode\n",
    "game_file = open(f'{folder_tmp_path}/{game_files[0]}', 'r')\n",
    "# using json loads to load the json data\n",
    "data = json.load(game_file)\n",
    "# catching up the key 'events' so we can go inside of the key values \n",
    "events = data['events']\n",
    "# create a list to store the moments keys (location, ball, player, team id)\n",
    "location_data = []\n",
    "# for each play in events, we are able to store the eventId and also the moments (with ball/team location info)\n",
    "for play in events:\n",
    "    # store the eventid\n",
    "    event_id = play['eventId']\n",
    "    # store the court info \n",
    "    court_info = play['moments']\n",
    "    # for each location info on court\n",
    "    for location in court_info:\n",
    "        # select the value where ball and player location are stored\n",
    "        for ball_or_player in location[5]:\n",
    "            # 'extend' allow create a list with multiple info, so we can load this into spark dataframe (horizontal data)\n",
    "            ball_or_player.extend((location[2], location[3], location[0], data['gameid'], event_id))\n",
    "            location_data.append(ball_or_player)\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1610612761,\n",
       " 2449,\n",
       " 19.08811,\n",
       " 13.91147,\n",
       " 0.0,\n",
       " 711.26,\n",
       " 11.99,\n",
       " 1,\n",
       " '0021500492',\n",
       " '1']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neon_team_data = spark.read.format(\"jdbc\") \\\n",
    ".option(\"url\", \"jdbc:postgresql://ep-rapid-cloud-796936.us-east-2.aws.neon.tech/neondb?user=joaopedro.brb&password=ymFheQfG70XC\") \\\n",
    ".option(\"dbtable\", \"teams_dimensions.team_data\") \\\n",
    ".option(\"user\", \"joaopedro.brb\") \\\n",
    ".option(\"password\", \"ymFheQfG70XC\") \\\n",
    ".option(\"driver\", \"org.postgresql.Driver\") \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_data.write.format(\"jdbc\").mode('append').option(\"url\", URL)\\\n",
    ".option(\"user\", USER)\\\n",
    ".option(\"password\", PASS)\\\n",
    ".option(\"dbtable\", 'teams_dimensions.team_data')\\\n",
    ".option(\"driver\", \"org.postgresql.Driver\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the game files from raw\n",
    "game_files = extract_files(folder_raw=folder_raw,folder_tmp_path=folder_tmp_path,urls=urls_list)     \n",
    "get_teams_data_dim = get_teams_data(game_files=game_files[i],folder_tmp_path=folder_tmp_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, LongType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.pandas as pd\n",
    "import os\n",
    "from urllib import request\n",
    "from py7zr import unpack_7zarchive\n",
    "import shutil\n",
    "import glob\n",
    "import json\n",
    "from script_etl import * \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "## WARNING: BEFORE RUNNING, PLEASE INSTALL THE FILE config/requirements.txt with command: pip install -r config/requirements.txt \n",
    "\n",
    "## Replace the next variables with your directory of preference ##\n",
    "folder_raw = '/workspace/nba_sas_assessment/raw_data' # directory to save and process raw data\n",
    "folder_tmp_path = \"/workspace/nba_sas_assessment/raw_data/tmp\" # directory to store json raw data\n",
    "\n",
    "## Replace the next variables with the path to postgres jar file and .env with credencials to access neon.tech ##\n",
    "jar_postgres_path = '/workspace/nba_sas_assessment/config/jar/postgresql-42.5.1.jar'\n",
    "env_postgres_credentials = '/workspace/nba_sas_assessment/index(6).env'\n",
    "\n",
    "# create spark session and return the session\n",
    "spark = create_spark_session(jar_file_path=jar_postgres_path)\n",
    "\n",
    "# create spark structure to received processed json data\n",
    "struct_spark = struct_field_create()\n",
    "\n",
    "## Here you have two options:\n",
    "        # 1) If you already had uploaded the file (.7z) inside the 'folder_raw', you can ignore the next variable containing a list\n",
    "        # 2) If you want to download one or more files and store into the 'folder_raw', fill the list 'urls_list' with the desired links to download\n",
    "\n",
    "urls_list = ['https://github.com/sealneaward/nba-movement-data/raw/master/data/01.01.2016.CHA.at.TOR.7z']\n",
    "\n",
    "\n",
    "# Extract the game files from raw\n",
    "game_files = extract_files(folder_raw=folder_raw,folder_tmp_path=folder_tmp_path,urls=urls_list)     \n",
    "\n",
    "# for each item on folder_tmp_path (each json file)\n",
    "for i in range(0,len(game_files)):\n",
    "    # Get data from teams\n",
    "    get_teams_data_dim = get_teams_data(game_files=game_files[i],folder_tmp_path=folder_tmp_path)                                                                                                                                                                                                                                       \n",
    "    # Get data from visitant team (players)\n",
    "    get_visitant_players_info = get_players_dimension_visitant(game_files=game_files[i],folder_tmp_path=folder_tmp_path)\n",
    "    # Get data from home team (players)\n",
    "    get_home_players_info = get_players_dimension_home(game_files=game_files[i],folder_tmp_path=folder_tmp_path)\n",
    "    # Get ball movement data\n",
    "    get_location_data = get_location_of_ball(game_files=game_files[i],folder_tmp_path=folder_tmp_path)\n",
    "    # Transform team data into dataframe\n",
    "    team_data = team_data_df(spark=spark,data_to_process=get_teams_data_dim,struct_spark=struct_spark)\n",
    "    # Transform visitant team (players) into dataframe\n",
    "    visitant_team = visitant_team_df(spark=spark,struct_spark=struct_spark,data_to_process=get_visitant_players_info)\n",
    "    # Transform home team (players) into dataframe\n",
    "    home_team = home_team_df(spark=spark,data_to_process=get_home_players_info,struct_spark=struct_spark)\n",
    "    # Transform ball movement into dataframe\n",
    "    location_of_the_ball = location_of_the_ball_df(spark=spark,data_to_process=get_location_data,struct_spark=struct_spark)\n",
    "    print(f'{env_postgres_credentials,jar_postgres_path}')\n",
    "    # Append all these dataframes into our database\n",
    "    append_to_postgres(location_data=location_of_the_ball\\\n",
    "                       ,home_data=home_team\\\n",
    "                       ,visitant_data=visitant_team\\\n",
    "                       ,team_data=team_data\\\n",
    "                       ,jar=jar_postgres_path\\\n",
    "                       ,cred=env_postgres_credentials)\n",
    "\n",
    "    # Remove JSON files to clean the folder\n",
    "    remove_json_file(path=folder_tmp_path,game_file=game_files[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create a folder to store our data if not exists\n",
    "folder = r'/workspace/nba_ball_movement/game_data' \n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "response_download_name = f'{folder}/game_data.7z'\n",
    "\n",
    "# download url file from git repo and save into folder created\n",
    "url_to_download = 'https://github.com/sealneaward/nba-movement-data/raw/master/data/01.02.2016.DET.at.IND.7z'\n",
    "response = request.urlretrieve(url_to_download, response_download_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py7zr import unpack_7zarchive\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# register unzip 7z function\n",
    "try:\n",
    "    shutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# list all files from download folder\n",
    "file_name = glob.glob(f'{folder}/*.7z')\n",
    "\n",
    "# create a count\n",
    "c = 0\n",
    "\n",
    "# path to output zipped files\n",
    "output_path = f'{folder}/tmp/'\n",
    "\n",
    "# for each file from download folder, decompress zipped and save into /tmp folder\n",
    "for i in file_name:\n",
    "    shutil.unpack_archive(file_name[0], output_path)\n",
    "    os.remove(file_name[0])\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# path where json is stored\n",
    "path = \"/workspace/nba_sas_assessment/raw_data/tmp\"\n",
    "\n",
    "# listing all files inside the path\n",
    "folder = os.listdir(path)\n",
    "# create a empty list to append if we have json files inside the folder\n",
    "game_files = []\n",
    "c = 0\n",
    "# for each information in folder\n",
    "for i in folder:\n",
    "    # if we have '.json' in string, append to our previous empty list\n",
    "    if '.json' in folder[c]:\n",
    "        game_files.append(folder[c])\n",
    "    c =+ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_home = 0\n",
    "c_visitant = 0\n",
    "c = 0\n",
    "# use python open method to open json file on read mode\n",
    "game_file = open(f'{path}/{game_files[0]}', 'r')\n",
    "# using json loads to load the json data\n",
    "data = json.load(game_file)\n",
    "data_home = data['events']\n",
    "# HOME TEAM INFO #\n",
    "player_data_home = []\n",
    "# fetching players of home team \n",
    "players_query = data_home[0]['home']['players']\n",
    "for i in players_query:\n",
    "    player_data_home.append([i for i in players_query[c_home].values()])\n",
    "    player_data_home[-1].extend((data['gameid'],data_home[0]['home']['teamid']))\n",
    "    c_home += 1\n",
    "# VISITANT TEAM INFO #\n",
    "player_data_visitant = []\n",
    "data_visitant = data['events']\n",
    "players_query = data_visitant[0]['visitor']['players']\n",
    "for i in players_query:\n",
    "    player_data_visitant.append([i for i in players_query[c_visitant].values()])\n",
    "    player_data_visitant[-1].extend((data['gameid'],data_visitant[0]['visitor']['teamid']))\n",
    "    c_visitant += 1    \n",
    "c =+ 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_file = open(f'{path}/{game_files[0]}', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(game_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('org.apache.spark.sql.json') \\\n",
    "        .load(f\"{path}/{game_files[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()\n",
    "\n",
    "schema_teams_info = StructType([ \\\n",
    "    StructField(\"game_id\",StringType(),True),\n",
    "    StructField(\"game_date\",StringType(),True),\n",
    "    StructField(\"events\",StringType(),True),\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(data, schema=schema_teams_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_file = open(f'{path}/{game_files[0]}', 'r')\n",
    "# using json loads to load the json data\n",
    "data = json.load(game_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = []\n",
    "teams.extend((data['events'][0]['home']['name'],data['events'][0]['home']['abbreviation'],data['events'][0]['home']['teamid'],data['events'][0]['visitor']['name'],data['events'][0]['visitor']['abbreviation'],data['events'][0]['visitor']['teamid'],data['gameid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_teams_info = StructType([ \\\n",
    "    StructField(\"home_team_name\",StringType(),True),\n",
    "    StructField(\"home_team_abbreviation\",StringType(),True),\n",
    "    StructField(\"home_team_id\",LongType(),True),\n",
    "    StructField(\"visitant_team_name\",StringType(),True),\n",
    "    StructField(\"visitant_team_abbreviation\", StringType(),True),\n",
    "    StructField(\"visitant_team_id\", LongType(),True),\n",
    "    StructField(\"game_id\", StringType(),True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "teams = [teams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_info_data = spark.createDataFrame(data=teams,schema=schema_teams_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_info_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gameid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_home = 0\n",
    "c_visitant = 0\n",
    "c = 0\n",
    "# use python open method to open json file on read mode\n",
    "game_file = open(f'{path}/{game_files[0]}', 'r')\n",
    "# using json loads to load the json data\n",
    "data = json.load(game_file)\n",
    "data_home = data['events']\n",
    "# HOME TEAM INFO #\n",
    "player_data_home = []\n",
    "# fetching players of home team \n",
    "players_query = data_home[0]['home']['players']\n",
    "for i in players_query:\n",
    "    player_data_home.append([i for i in players_query[c_home].values()])\n",
    "    player_data_home[-1].extend((data['gameid'],data_home[0]['home']['teamid']))\n",
    "    c_home += 1\n",
    "# VISITANT TEAM INFO #\n",
    "player_data_visitant = []\n",
    "data_visitant = data['events']\n",
    "players_query = data_visitant[0]['visitor']['players']\n",
    "for i in players_query:\n",
    "    player_data_visitant.append([i for i in players_query[c_visitant].values()])\n",
    "    player_data_visitant[-1].extend((data['gameid'],data_visitant[0]['visitor']['teamid']))\n",
    "    c_visitant += 1    \n",
    "c =+ 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_data_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our schema to upload data\n",
    "schema_location_of_ball_and_teams = StructType([ \\\n",
    "    StructField(\"team_id\",LongType(),True),\n",
    "    StructField(\"player_id\",LongType(),True),\n",
    "    StructField(\"x_loc\",FloatType(),True),\n",
    "    StructField(\"y_loc\",FloatType(),True),\n",
    "    StructField(\"radius\", FloatType(),True),\n",
    "    StructField(\"game_clock\", FloatType(),True),\n",
    "    StructField(\"shot_clock\", FloatType(),True),\n",
    "    StructField(\"quarter\", IntegerType(),True),\n",
    "    StructField(\"game_id\", StringType(),True),\n",
    "    StructField(\"event_id\", StringType(),True),\n",
    "    ]\n",
    "  )\n",
    "\n",
    "schema_team_info = StructType([ \\\n",
    "\n",
    "    StructField(\"last_name\",StringType(),True),\n",
    "    StructField(\"first_name\",StringType(),True),\n",
    "    StructField(\"player_id\",StringType(),True),\n",
    "    StructField(\"team_id\",StringType(),True),\n",
    "    StructField(\"jersey_number\",StringType(),True),\n",
    "    StructField(\"position\", StringType(),True),\n",
    "    StructField(\"game_id\", StringType(),True)\n",
    "    ]\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the ball movement dataframe\n",
    "#game_ball_movement = spark.createDataFrame(data=location_data, schema=schema_location_of_ball_and_teams)\n",
    "\n",
    "# create the home team players dimension\n",
    "home_team_data = spark.createDataFrame(data=player_data_home, schema=schema_team_info) \n",
    "\n",
    "# create the visitant team players dimension\n",
    "#visitant_team_data = spark.createDataFrame(data=player_data_visitant, schema=schema_team_info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = spark.read.format(\"jdbc\").option(\"url\", URL)\\\n",
    ".option(\"user\", USER)\\\n",
    ".option(\"password\", PASS)\\\n",
    ".option(\"query\", 'select distinct * from teams_dimensions.players_data')\\\n",
    ".option(\"driver\", \"org.postgresql.Driver\")\\\n",
    ".load()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('players_neon')\n",
    "home_team_data('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.union(home_team_data).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_team_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_team_data.write.format(\"jdbc\").mode('ignore').option(\"url\", URL)\\\n",
    ".option(\"user\", USER)\\\n",
    ".option(\"password\", PASS)\\\n",
    ".option(\"dbtable\", 'teams_dimensions.team_data')\\\n",
    ".option(\"driver\", \"org.postgresql.Driver\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "env_postgres_credentials = './workspace/nba_sas_assessment/config/postgres_login.env'\n",
    "path = env_postgres_credentials\n",
    "\n",
    "load_dotenv(path)\n",
    "\n",
    "USER = os.getenv('PGUSER')\n",
    "PASS = os.getenv('PGPASSWORD')\n",
    "\n",
    "credentials = {\n",
    "    'url': os.getenv('JDBC'),\n",
    "    'user': USER,\n",
    "    'password': PASS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_postgres_credentials = './workspace/nba_sas_assessment/config/postgres_login.env'\n",
    "credentials = env_postgres_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(credentials)\n",
    "URL = os.getenv('JDBC')\n",
    "USER = os.getenv('PGUSER')\n",
    "PASS = os.getenv('PGPASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = spark.read.format(\"jdbc\").option(\"url\", URL)\\\n",
    ".option(\"user\", USER)\\\n",
    ".option(\"password\", PASS)\\\n",
    ".option(\"dbtable\", 'teams_dimensions.team_data')\\\n",
    ".option(\"driver\", \"org.postgresql.Driver\")\\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6823027d7e29b41a4e6f28161e95b4dfbe0e8b9a6a7278f50ef7cf75c6459dbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
